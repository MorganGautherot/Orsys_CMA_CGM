{"cells":[{"cell_type":"markdown","metadata":{"id":"lCz5SCTZUMAK"},"source":["# Entra√Æner un mod√®le Transformer pour la traduction\n","\n","**Objectif p√©dagogique :** Apprendre √† construire et entra√Æner un mod√®le de traduction automatique Anglais ‚Üí Fran√ßais"]},{"cell_type":"markdown","metadata":{"id":"b3Gy_hd2UMAN"},"source":["## Installation et imports\n","\n","**Objectif :** Installer les biblioth√®ques modernes pour le deep learning et le NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9guei-XUMAO"},"outputs":[],"source":["# Installation des packages modernes (ex√©cuter une seule fois)\n","# !pip install transformers datasets torch torchvision torchaudio\n","# !pip install pandas matplotlib tqdm scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uETGrpfkUMAP"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Biblioth√®ques pour NLP moderne\n","from transformers import AutoTokenizer\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tqdm.auto import tqdm\n","import math\n","import os\n","from typing import List\n","from timeit import default_timer as timer"]},{"cell_type":"markdown","metadata":{"id":"IfcoBjwaUMAQ"},"source":["## Configuration et d√©tection mat√©riel\n","\n","**Concept important :** Il est crucial de d√©tecter si un GPU est disponible pour acc√©l√©rer l'entra√Ænement !"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnMSj1toUMAQ"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"üñ•Ô∏è  Device utilis√©: {DEVICE}\")\n","\n","# Configuration de la reproductibilit√©\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    # Configuration pour performances reproductibles sur GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"QBu0-NZgUMAQ"},"source":["## Hyperparam√®tres"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5NTxcmxUMAR"},"outputs":[],"source":["EMB_SIZE = 256  # Dimension des embeddings (doit √™tre divisible par NHEAD)\n","NHEAD = 8       # Nombre de t√™tes d'attention (8 divise 256)\n","FFN_HID_DIM = 512  # Dimension de la couche feed-forward\n","\n","# Adaptation de la batch size selon les capacit√©s du device\n","BATCH_SIZE = 64 if DEVICE.type == 'cuda' else 32  # GPU peut traiter plus de donn√©es\n","\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","NUM_EPOCHS = 15  # Nombre d'√©poques d'entra√Ænement\n","LEARNING_RATE = 0.0001  # Taux d'apprentissage conservateur\n","\n","print(f\"üìä Batch size: {BATCH_SIZE}\")\n","print(f\"üß† Embedding size: {EMB_SIZE}\")\n","print(f\"üëÅÔ∏è  Attention heads: {NHEAD}\")\n","print(f\"üî• Epochs: {NUM_EPOCHS}\")"]},{"cell_type":"markdown","metadata":{"id":"_l07KN_cUMAR"},"source":["## Chargement des donn√©es\n","\n","**Dataset :** Nous utilisons un dataset de phrases parall√®les Anglais-Fran√ßais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhOQoT6yUMAR"},"outputs":[],"source":["# Chargement du dataset (fourni)\n","try:\n","    import kagglehub\n","    path = kagglehub.dataset_download(\"devicharith/language-translation-englishfrench\")\n","    csv_path = os.path.join(path, 'eng_-french.csv')\n","except:\n","    # Alternative: mettre votre chemin local\n","    csv_path = 'eng_-french.csv'\n","\n","# Chargement avec pandas et s√©lection des colonnes n√©cessaires\n","df = pd.read_csv(\n","    csv_path,\n","    usecols=['English words/sentences', 'French words/sentences']\n",")\n","df = df.dropna()  # Supprimer les valeurs manquantes\n","\n","print(f\"üìä Dataset size: {len(df)} exemples\")\n","# Affichage des premi√®res lignes pour v√©rification\n","print(df.head())"]},{"cell_type":"markdown","metadata":{"id":"upPNRoc1UMAR"},"source":["## Tokenisation moderne\n","\n","**Concept :** Les tokenizers pr√©-entra√Æn√©s sont plus efficaces que construire le sien from scratch\n","\n","Les tokenizers pr√©-entra√Æn√©s sont construits √† partir de corpus massifs et vari√©s, ce qui leur permet de produire une segmentation plus coh√©rente et plus compacte que celle obtenue √† partir de donn√©es limit√©es.\n","Cette segmentation r√©duit le nombre de tokens par texte, ce qui diminue le co√ªt de calcul et am√©liore l‚Äôutilisation du contexte par le mod√®le.\n","Ils sont aussi parfaitement align√©s avec les embeddings et les architectures existantes, garantissant une compatibilit√© et des performances optimales.\n","Construire son tokenizer from scratch introduit donc souvent une segmentation sous-optimale, de la dette technique et une perte de performance globale."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVfhHwMOUMAS"},"outputs":[],"source":["# TODO: Initialisez les tokenizers pour anglais et fran√ßais\n","# Aide: utilisez AutoTokenizer.from_pretrained() avec \"bert-base-uncased\" pour EN et \"camembert-base\" pour FR\n","\n","from transformers import AutoTokenizer\n","\n","tokenizer_en = # TODO: Tokenizer anglais\n","tokenizer_fr = # TODO: Tokenizer fran√ßais\n","\n","# Test des tokenizers\n","test_en = \"Hello I love machine learning\"\n","test_fr = \"Bonjour j'adore l'apprentissage automatique\"\n","\n","print(f\"üî§ Tokenization EN: {tokenizer_en.tokenize(test_en)}\")\n","print(f\"üî§ Tokenization FR: {tokenizer_fr.tokenize(test_fr)}\")\n","\n","print(\"‚úÖ Tokenizers initialis√©s !\")"]},{"cell_type":"markdown","metadata":{"id":"0iNAnooTUMAS"},"source":["## Construction du vocabulaire"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9UEoSgiUMAS"},"outputs":[],"source":["# Tokens sp√©ciaux (fourni)\n","PAD_IDX = 0  # Padding\n","BOS_IDX = 1  # Beginning of Sentence\n","EOS_IDX = 2  # End of Sentence\n","UNK_IDX = 3  # Unknown word\n","\n","def build_vocab(sentences, tokenizer, max_vocab=10000):\n","    \"\"\"Construction du vocabulaire √† partir des phrases\"\"\"\n","    word_count = {}\n","\n","    # TODO: Parcourez toutes les phrases et comptez les mots\n","    for sentence in sentences:\n","        tokens = tokenizer.tokenize(str(sentence))\n","        for token in tokens:\n","            # TODO: Incr√©mentez le compteur pour ce token en utilisant word_count.get()\n","            word_count[token] = # TODO: Compl√©tez\n","\n","    # TODO: Triez les mots par fr√©quence (du plus fr√©quent au moins fr√©quent)\n","    # Utilisez la fonction sorted() avec une key appropri√©e\n","    sorted_words = # TODO: utilisez sorted() avec une key appropri√©e\n","\n","    # Cr√©ation du vocabulaire avec tokens sp√©ciaux\n","    vocab = {\"<pad>\": PAD_IDX, \"<bos>\": BOS_IDX, \"<eos>\": EOS_IDX, \"<unk>\": UNK_IDX}\n","\n","    # TODO: Ajoutez les mots les plus fr√©quents au vocabulaire\n","    for word, _ in sorted_words[:max_vocab-4]:  # -4 pour les tokens sp√©ciaux\n","        vocab[word] = # TODO: Compl√©tez avec la taille actuelle du vocabulaire\n","\n","    return vocab\n","\n","# Construction des vocabulaires\n","vocab_en = build_vocab(df['English words/sentences'], tokenizer_en)\n","vocab_fr = build_vocab(df['French words/sentences'], tokenizer_fr)\n","\n","print(f\"üìñ Vocabulaire EN: {len(vocab_en)} mots\")\n","print(f\"üìñ Vocabulaire FR: {len(vocab_fr)} mots\")"]},{"cell_type":"markdown","metadata":{"id":"-OVYNpBiUMAS"},"source":["## Dataset personnalis√©\n","\n","**Architecture :** PyTorch utilise des classes Dataset pour organiser les donn√©es"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOkqYnifUMAS"},"outputs":[],"source":["def text_to_indices(text, tokenizer, vocab, max_len=50):\n","    \"\"\"Convertit un texte en indices avec BOS et EOS\"\"\"\n","    tokens = tokenizer.tokenize(str(text))[:max_len-2]  # Place pour BOS/EOS\n","\n","    # TODO: Convertissez les tokens en indices\n","    # Aide: utilisez vocab.get(token, UNK_IDX) pour g√©rer les mots inconnus\n","    indices = [BOS_IDX] +  [TODO: liste en compr√©hension] + [EOS_IDX]\n","\n","    return torch.tensor(indices, dtype=torch.long)\n","\n","class TranslationDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df.reset_index(drop=True)\n","\n","    def __len__(self):\n","        # TODO: Retournez la taille du dataset\n","        return # TODO\n","\n","    def __getitem__(self, idx):\n","        # TODO: R√©cup√©rez les textes anglais et fran√ßais √† partir de self.df\n","        en_text = # TODO\n","        fr_text = # TODO\n","\n","        # TODO: Convertissez en indices en utilisant text_to_indices\n","        en_indices = # TODO\n","        fr_indices = # TODO\n","\n","        return en_indices, fr_indices\n","\n","print(\"‚úÖ Classes Dataset d√©finies !\")"]},{"cell_type":"markdown","metadata":{"id":"KmedX6XeUMAS"},"source":["## DataLoaders\n","\n","**Concept :** Le padding permet de traiter des phrases de longueurs diff√©rentes dans le m√™me batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgbNsPjRUMAS"},"outputs":[],"source":["def collate_batch(batch):\n","    \"\"\"Fonction pour cr√©er des batchs avec padding\"\"\"\n","    src_batch, tgt_batch = zip(*batch)\n","\n","    # TODO: Utilisez pad_sequence pour padding des s√©quences\n","    # Aide: padding_value=PAD_IDX, batch_first=True\n","    src_batch = # TODO\n","    tgt_batch = # TODO\n","\n","    return src_batch, tgt_batch\n","\n","# TODO: Divisez les donn√©es en train/test\n","# Aide: utilisez train_test_split avec test_size=0.1\n","train_df, test_df = # TODO\n","\n","# TODO: Cr√©ez les datasets avec TranslationDataset\n","train_dataset = # TODO\n","test_dataset = # TODO\n","\n","# TODO: Cr√©ez les DataLoaders\n","# Aide: batch_size=BATCH_SIZE, shuffle=True pour train, collate_fn=collate_batch\n","train_loader = DataLoader(# TODO)\n","test_loader = DataLoader(# TODO)\n","\n","print(f\"üîÑ Training batches: {len(train_loader)}\")\n","print(f\"üîÑ Test batches: {len(test_loader)}\")"]},{"cell_type":"markdown","metadata":{"id":"B_yKFKMfUMAT"},"source":["## Architecture du mod√®le Transformer\n","\n","**Architecture :** Le Transformer utilise l'attention pour \"encoder\" les relations entre mots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHJwHRyYUMAT"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"Encodage positionnel pour que le mod√®le comprenne l'ordre des mots\"\"\"\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # TODO: Cr√©ez le tensor d'encodage positionnel\n","        # uniquement des z√©ros au d√©but\n","        pe = # TODO\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","\n","        # Formule math√©matique pour l'encodage positionnel\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term) # Positions paires: sin\n","        pe[:, 1::2] = torch.cos(position * div_term) # Positions impaires: cos\n","        pe = pe.unsqueeze(0)\n","\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # TODO: Ajoutez l'encodage positionnel aux embeddings\n","        # Attention: prendre seulement la longueur n√©cessaire\n","        x = x + # TODO\n","        return self.dropout(x)\n","\n","print(\"‚úÖ PositionalEncoding d√©fini !\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYBR9LK-UMAT"},"outputs":[],"source":["class TransformerTranslator(nn.Module):\n","    \"\"\"Mod√®le Transformer complet pour la traduction\"\"\"\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,\n","                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","\n","        # TODO: Cr√©ez les couches d'embedding pour source et target\n","        # Aide: nn.Embedding(vocab_size, d_model)\n","        self.src_embedding = # TODO\n","        self.tgt_embedding = # TODO\n","\n","        # TODO: Ajoutez l'encodage positionnel avec PostionalEncoding\n","        self.pos_encoding = # TODO\n","\n","        # TODO: Cr√©ez le Transformer\n","        # Aide: nn.Transformer avec tous les param√®tres, batch_first=True\n","        self.transformer = # TODO\n","\n","        # TODO: Couche de sortie pour pr√©dire les mots\n","        # Aide: nn.Linear(d_model, tgt_vocab_size)\n","        self.output_projection = # TODO\n","\n","        # Initialisation des poids (bonne pratique)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        \"\"\"Initialisation Xavier/Glorot pour meilleure convergence\n","        PyTorch utilise par d√©faut : Normal(0, 1) pour les embeddings\n","        PyTorch utilise : Uniform(-1/sqrt(in_features), 1/sqrt(in_features)) pour les Linear\n","        Les embeddings avec Normal(0,1) peuvent √™tre trop grands Uniform(-0.1, 0.1) plus stable pour l'attention\"\"\"\n","        initrange = 0.1\n","        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n","        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n","        self.output_projection.bias.data.zero_()\n","        self.output_projection.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n","                src_key_padding_mask=None, tgt_key_padding_mask=None):\n","\n","        # TODO: Appliquez les embeddings avec scaling et encodage positionnel\n","        # Aide: embedding * sqrt(d_model), puis pos_encoding\n","        src_emb = # TODO\n","        tgt_emb = # TODO\n","\n","        # TODO: Passez dans le Transformer\n","        output = # TODO\n","\n","        # TODO: Retournez la projection finale\n","        return # TODO\n","\n","print(\"‚úÖ Mod√®le Transformer d√©fini !\")"]},{"cell_type":"markdown","metadata":{"id":"Tgg0UP9aUMAT"},"source":["## Cr√©ation des masques\n","\n","**Concept crucial :** Les masques emp√™chent le mod√®le de \"tricher\" en regardant les mots futurs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2dOT6nwUMAT"},"outputs":[],"source":["def create_masks(src, tgt):\n","    \"\"\"Cr√©e les masques n√©cessaires pour l'attention\"\"\"\n","    src_seq_len = src.shape[1]\n","    tgt_seq_len = tgt.shape[1]\n","\n","    # Emp√™che le d√©codeur de voir les mots futurs\n","    tgt_mask = torch.triu(\n","        torch.ones(tgt_seq_len, tgt_seq_len, device=src.device) * float('-inf'),\n","        diagonal=1\n","    ).bool()\n","\n","    # TODO: Cr√©ez les masques de padding\n","    # Aide: (src == PAD_IDX) pour identifier les tokens de padding\n","    src_padding_mask = # TODO\n","    tgt_padding_mask = # TODO\n","\n","    return None, tgt_mask, src_padding_mask, tgt_padding_mask\n","\n","print(\"‚úÖ Fonction de masques d√©finie !\")"]},{"cell_type":"markdown","metadata":{"id":"d0VXn1AnUMAT"},"source":["## Initialisation du mod√®le"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4jgSVlsUMAT"},"outputs":[],"source":["# TODO: Initialisez le mod√®le avec vos hyperparam√®tres\n","model = # TODO\n","# N'oubliez pas de mettre sur le bon device !\n","\n","# TODO: Comptez les param√®tres\n","total_params = sum(# TODO: utilisez p.numel() pour compter tous les param√®tres)\n","trainable_params = sum(# TODO: pareil mais avec if p.requires_grad)\n","\n","print(f\"üßÆ Param√®tres totaux: {total_params:,}\")\n","print(f\"üßÆ Param√®tres entra√Ænables: {trainable_params:,}\")\n","\n","# Estimation m√©moire approximative\n","memory_mb = (total_params * 4) / (1024**2)  # 4 bytes par param√®tre float32\n","print(f\"üíæ M√©moire approximative: {memory_mb:.1f} MB\")"]},{"cell_type":"markdown","metadata":{"id":"TIm4lHqbUMAT"},"source":["## Configuration de l'entra√Ænement\n","\n","**Optimisation :** Le gradient clipping √©vite les explosions de gradient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsteRHqBUMAU"},"outputs":[],"source":["# TODO: D√©finissez la fonction de co√ªt\n","# Aide: nn.CrossEntropyLoss avec ignore_index=PAD_IDX\n","criterion = # TODO\n","\n","# TODO: D√©finissez l'optimiseur\n","# Aide: torch.optim.AdamW avec lr=LEARNING_RATE betas=(0.9, 0.98), eps=1e-9 et weight_decay=0.01\n","optimizer = # TODO\n","\n","# TODO: Ajoutez un scheduler\n","# Aide: torch.optim.lr_scheduler.StepLR avec step_size=5 et gamma=0.7\n","scheduler = # TODO\n","\n","print(\"‚úÖ Optimiseurs configur√©s !\")"]},{"cell_type":"markdown","metadata":{"id":"3XzecpqbUMAU"},"source":["## Fonctions d'entra√Ænement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GkPqg91UMAU"},"outputs":[],"source":["def train_epoch(model, train_loader, criterion, optimizer, device):\n","    \"\"\"Entra√Æne le mod√®le pour une √©poque\"\"\"\n","    # TODO: Mettez le mod√®le en mode entra√Ænement avec la m√©thode .train()\n","    model.# TODO\n","\n","    total_loss = 0\n","    num_batches = len(train_loader)\n","\n","    for src, tgt in tqdm(train_loader, desc=\"Training\"):\n","        # TODO: D√©placez les donn√©es sur le bon device avec .to(device)\n","        src, tgt = # TODO\n","\n","        # TODO: Pr√©parez les donn√©es (input = tout sauf dernier, output = tout sauf premier)\n","        tgt_input = # TODO\n","        tgt_output = # TODO\n","\n","        # TODO: Cr√©ez les masques avec create_masks\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = # TODO\n","\n","        # TODO: Forward pass\n","        logits = model(# TODO: tous les arguments n√©cessaires)\n","\n","        # TODO: Calculez la loss\n","        loss = criterion(# TODO: reshape appropri√© pour logits et tgt_output)\n","\n","        # TODO: Backward pass avec gradient clipping\n","        # Reset gradients avec zero_grad()\n","        optimizer.#TODO\n","        # Calcul gradients avec .backward()\n","        loss.#TODO\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / num_batches\n","\n","print(\"‚úÖ Fonction train_epoch d√©finie !\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Brn5aAK4UMAU"},"outputs":[],"source":["def evaluate(model, test_loader, criterion, device):\n","    \"\"\"√âvalue le mod√®le sur les donn√©es de test\"\"\"\n","    # TODO: Mettez le mod√®le en mode √©valuation\n","    model.# TODO\n","\n","    total_loss = 0\n","    num_batches = len(test_loader)\n","\n","    # TODO: Utilisez torch.no_grad() dans le with pour √©conomiser la m√©moire\n","    with # TODO:\n","        for src, tgt in tqdm(test_loader, desc=\"Evaluating\"):\n","\n","            src, tgt = src.to(device), tgt.to(device)\n","\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","\n","            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input)\n","\n","            logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\n","            loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))\n","\n","            total_loss += loss.item()\n","\n","    return total_loss / num_batches\n","\n","print(\"‚úÖ Fonction evaluate d√©finie !\")"]},{"cell_type":"markdown","metadata":{"id":"Qg9i98ptUMAU"},"source":["## Boucle d'entra√Ænement principale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cptuJBiUMAU"},"outputs":[],"source":["# Listes pour stocker l'historique\n","train_losses = []\n","val_losses = []\n","best_val_loss = float('inf')\n","\n","print(\"üöÄ D√©but de l'entra√Ænement !\")\n","print(\"=\" * 60)\n","\n","for epoch in range(NUM_EPOCHS):\n","    start_time = timer()\n","\n","    # TODO: Entra√Ænement\n","    train_loss = train_epoch(# TODO: arguments)\n","\n","    # TODO: Validation\n","    val_loss = evaluate(# TODO: arguments)\n","\n","    scheduler.step()\n","\n","    end_time = timer()\n","\n","    # Sauvegarde de l'historique\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","\n","    # Affichage des r√©sultats\n","    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n","    print(f\"  üìä Train Loss: {train_loss:.4f}\")\n","    print(f\"  üìä Val Loss: {val_loss:.4f}\")\n","    print(f\"  ‚è±Ô∏è  Time: {end_time - start_time:.2f}s\")\n","    print(f\"  üéöÔ∏è  LR: {scheduler.get_last_lr()[0]:.6f}\")\n","\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'val_loss': val_loss,\n","            'vocab_en': vocab_en,\n","            'vocab_fr': vocab_fr\n","        }, 'best_model_students.pt')\n","        print(f\"  üéØ Nouveau meilleur mod√®le sauvegard√©!\")\n","\n","    print(\"-\" * 50)\n","\n","print(\"‚úÖ Entra√Ænement termin√© !\")"]},{"cell_type":"markdown","metadata":{"id":"MHZgkdJWUMAU"},"source":["## Visualisation des r√©sultats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NzS_E9TPUMAU"},"outputs":[],"source":["\n","plt.figure(figsize=(12, 8))\n","\n","# Subplot 1: Courbes de loss\n","plt.subplot(2, 2, 1)\n","plt.plot(range(1, len(train_losses) + 1), train_losses,\n","         label='Train Loss', color='blue', linewidth=2)\n","plt.plot(range(1, len(val_losses) + 1), val_losses,\n","         label='Validation Loss', color='red', linewidth=2)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('√âvolution des losses pendant l\\'entra√Ænement')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# Subplot 2: Diff√©rence train/val (d√©tection overfitting)\n","plt.subplot(2, 2, 2)\n","diff_losses = [t - v for t, v in zip(train_losses, val_losses)]\n","plt.plot(range(1, len(diff_losses) + 1), diff_losses,\n","         color='orange', linewidth=2)\n","plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n","plt.xlabel('Epochs')\n","plt.ylabel('Train Loss - Val Loss')\n","plt.title('√âcart Train/Validation (Overfitting)')\n","plt.grid(True, alpha=0.3)\n","\n","# Subplot 3: Learning rate √©volution\n","plt.subplot(2, 2, 3)\n","lrs = [LEARNING_RATE * (scheduler.gamma ** (i // scheduler.step_size))\n","       for i in range(len(train_losses))]\n","plt.plot(range(1, len(lrs) + 1), lrs, color='green', linewidth=2)\n","plt.xlabel('Epochs')\n","plt.ylabel('Learning Rate')\n","plt.title('√âvolution du Learning Rate')\n","plt.yscale('log')  # √âchelle logarithmique pour mieux voir\n","plt.grid(True, alpha=0.3)\n","\n","# Subplot 4: Am√©lioration par √©poque\n","plt.subplot(2, 2, 4)\n","val_improvements = [0] + [val_losses[i-1] - val_losses[i]\n","                          for i in range(1, len(val_losses))]\n","colors = ['green' if x > 0 else 'red' for x in val_improvements]\n","plt.bar(range(1, len(val_improvements) + 1), val_improvements, color=colors, alpha=0.7)\n","plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n","plt.xlabel('Epochs')\n","plt.ylabel('Am√©lioration Val Loss')\n","plt.title('Am√©lioration par √âpoque')\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Sauvegarde du graphique\n","plt.savefig('training_analysis_correction.png', dpi=300, bbox_inches='tight')\n","\n","# Analyse automatique des r√©sultats\n","print(\"üìà Analyse automatique des r√©sultats:\")\n","print(\"=\" * 50)\n","print(f\"  üèÜ Meilleure val loss: {best_val_loss:.4f}\")\n","print(f\"  üìä Loss finale train: {train_losses[-1]:.4f}\")\n","print(f\"  üìä Loss finale val: {val_losses[-1]:.4f}\")\n","print(f\"  üìâ R√©duction train loss: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")\n","print(f\"  üìâ R√©duction val loss: {((val_losses[0] - val_losses[-1]) / val_losses[0] * 100):.1f}%\")\n","\n","# D√©tection automatique des probl√®mes\n","final_gap = train_losses[-1] - val_losses[-1]\n","if final_gap > 0.5:\n","    print(f\"  ‚ö†Ô∏è  OVERFITTING d√©tect√© (√©cart: {final_gap:.3f})\")\n","elif final_gap < -0.2:\n","    print(f\"  ‚ö†Ô∏è  UNDERFITTING possible (√©cart: {final_gap:.3f})\")\n","else:\n","    print(f\"  ‚úÖ √âquilibre train/val correct (√©cart: {final_gap:.3f})\")\n","\n","# Convergence\n","if len(val_losses) > 5:\n","    recent_std = np.std(val_losses[-5:])\n","    if recent_std < 0.01:\n","        print(f\"  ‚úÖ Convergence stable (std r√©cente: {recent_std:.4f})\")\n","    else:\n","        print(f\"  üìä Convergence en cours (std r√©cente: {recent_std:.4f})\")"]},{"cell_type":"markdown","metadata":{"id":"q-PjuSZKUMAU"},"source":["## Test du mod√®le\n","\n","**Le moment de v√©rit√© :** Votre mod√®le sait-il traduire ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-tfI68BUMAV"},"outputs":[],"source":["checkpoint = torch.load('best_model_students_correction.pt', map_location=DEVICE)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(f\"‚úÖ Meilleur mod√®le charg√© (√©poque {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n","\n","# Vocabulaire invers√© pour la conversion\n","idx_to_word_fr = {idx: word for word, idx in vocab_fr.items()}\n","\n","def translate_sentence(model, sentence, max_length=50, verbose=False):\n","    \"\"\"Traduit une phrase anglaise en fran√ßais avec d√©codage greedy\"\"\"\n","    model.eval()\n","\n","    with torch.no_grad():\n","        # Pr√©processing de la phrase source\n","        src_indices = text_to_indices(sentence, tokenizer_en, vocab_en)\n","        src = src_indices.unsqueeze(0).to(DEVICE)\n","\n","        if verbose:\n","            print(f\"Source tokens: {[vocab_en_inv.get(idx.item(), '<unk>') for idx in src_indices]}\")\n","\n","        # Initialisation avec BOS token\n","        tgt = torch.tensor([[BOS_IDX]], device=DEVICE)\n","\n","        # G√©n√©ration auto-r√©gressive mot par mot\n","        for step in range(max_length):\n","            # Cr√©ation des masques pour l'√©tat actuel\n","            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt)\n","\n","            # Pr√©diction du prochain token\n","            logits = model(\n","                src, tgt,\n","                src_mask, tgt_mask,\n","                src_padding_mask, tgt_padding_mask\n","            )\n","\n","            # S√©lection du token le plus probable (greedy)\n","            next_token = logits[0, -1].argmax().unsqueeze(0).unsqueeze(0)\n","\n","            if verbose:\n","                probs = F.softmax(logits[0, -1], dim=0)\n","                top_tokens = probs.topk(3)\n","                print(f\"Step {step}: Top tokens: {[(idx_to_word_fr.get(idx.item(), '<unk>'), prob.item()) for idx, prob in zip(top_tokens.indices, top_tokens.values)]}\")\n","\n","            # Arr√™t si EOS g√©n√©r√©\n","            if next_token.item() == EOS_IDX:\n","                break\n","\n","            # Ajout du nouveau token √† la s√©quence\n","            tgt = torch.cat([tgt, next_token], dim=1)\n","\n","        # Conversion des indices vers les mots\n","        translation = []\n","        for idx in tgt[0][1:]:  # Ignorer BOS au d√©but\n","            word = idx_to_word_fr.get(idx.item(), '<unk>')\n","            if word in ['<eos>', '<pad>']:\n","                break\n","            translation.append(word)\n","\n","        # Post-processing basique pour am√©liorer la lisibilit√©\n","        result = ' '.join(translation)\n","        # Corrections de ponctuation basiques\n","        result = result.replace(' ,', ',').replace(' .', '.').replace(' !', '!').replace(' ?', '?').replace('_', '')\n","\n","        return result\n","\n","print(\"‚úÖ Fonction de traduction d√©finie !\")\n","\n","# Vocabulaire invers√© pour debug\n","vocab_en_inv = {idx: word for word, idx in vocab_en.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8XhYVarHUMAV"},"outputs":[],"source":["# Tests complets avec analyse de qualit√©\n","\n","test_sentences = [\n","    \"Hello, how are you?\",\n","    \"I love machine learning.\",\n","    \"The weather is beautiful today.\",\n","    \"Thank you very much.\",\n","    \"Good morning!\",\n","    \"What is your name?\",\n","    \"I am learning French.\",\n","    \"The book is on the table.\",\n","    \"Where are you going?\",\n","    \"This is a difficult problem.\"\n","]\n","\n","# Traductions de r√©f√©rence pour comparaison\n","references = [\n","    \"Bonjour, comment allez-vous ?\",\n","    \"J'adore l'apprentissage automatique.\",\n","    \"Le temps est beau aujourd'hui.\",\n","    \"Merci beaucoup.\",\n","    \"Bonjour !\",\n","    \"Comment vous appelez-vous ?\",\n","    \"J'apprends le fran√ßais.\",\n","    \"Le livre est sur la table.\",\n","    \"O√π allez-vous ?\",\n","    \"C'est un probl√®me difficile.\"\n","]\n","\n","print(\"üéØ Test de traduction avec √©valuation:\")\n","print(\"=\" * 70)\n","\n","total_score = 0\n","for i, sentence in enumerate(test_sentences):\n","    translation = translate_sentence(model, sentence)\n","\n","    print(f\"\\nüìù Test {i+1}:\")\n","    print(f\"üá¨üáß EN: {sentence}\")\n","    print(f\"ü§ñ AI: {translation}\")\n","    print(f\"üìö REF: {references[i]}\")\n","\n","    # √âvaluation qualitative simple\n","    prediction_words = set(translation.lower().split())\n","    reference_words = set(references[i].lower().split())\n","\n","    if prediction_words and reference_words:\n","        overlap = len(prediction_words & reference_words)\n","        union = len(prediction_words | reference_words)\n","        jaccard = overlap / union if union > 0 else 0\n","        total_score += jaccard\n","\n","        if jaccard > 0.6:\n","            quality = \"üü¢ Excellent\"\n","        elif jaccard > 0.4:\n","            quality = \"üü° Correct\"\n","        elif jaccard > 0.2:\n","            quality = \"üü† Partiel\"\n","        else:\n","            quality = \"üî¥ Faible\"\n","\n","        print(f\"üìä Similarit√©: {jaccard:.2f} {quality}\")\n","\n","    print(\"-\" * 40)\n","\n","average_score = total_score / len(test_sentences)\n","print(f\"\\nüèÜ Score moyen: {average_score:.3f}\")\n","\n","if average_score > 0.5:\n","    print(\"üéâ Performance globale: BONNE !\")\n","elif average_score > 0.3:\n","    print(\"üëç Performance globale: Correcte\")\n","else:\n","    print(\"üìö Performance globale: √Ä am√©liorer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85AVw3JVUMAV"},"outputs":[],"source":["# Test interactif\n","\n","print(\"‚ú® Test interactif - Ajoutez vos propres phrases:\")\n","print(\"=\" * 50)\n","\n","# Vos phrases personnalis√©es que vous pouvez modifier\n","custom_sentences = [\n","    \"The cat is sleeping on the sofa.\",\n","    \"I want to travel to Paris.\",\n","    \"Can you help me please?\",\n","    \"The students are working hard.\",\n","    \"Technology is changing the world.\"\n","]\n","\n","for i, sentence in enumerate(custom_sentences):\n","    print(f\"\\nüß™ Test personnalis√© {i+1}:\")\n","    translation = translate_sentence(model, sentence)\n","    print(f\"üá¨üáß EN: {sentence}\")\n","    print(f\"üá´üá∑ FR: {translation}\")\n","\n","    # Analyse d√©taill√©e pour 1 exemple\n","    if i == 0:\n","        print(\"\\nüîç Analyse d√©taill√©e:\")\n","        detailed_translation = translate_sentence(model, sentence, verbose=True)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}