{"cells":[{"cell_type":"markdown","metadata":{"id":"MdJGI5d7QpXO"},"source":["# Entra√Æner un mod√®le Transformer pour la traduction - CORRECTION\n","\n","**Objectif p√©dagogique :** Apprendre √† construire et entra√Æner un mod√®le de traduction automatique Anglais ‚Üí Fran√ßais"]},{"cell_type":"markdown","metadata":{"id":"fg7VH8ivQpXU"},"source":["## Installation et imports\n","\n","**Objectif :** Installer les biblioth√®ques modernes pour le deep learning et le NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0z6tLpxyQpXV"},"outputs":[],"source":["# Installation des packages modernes (ex√©cuter une seule fois)\n","!pip install transformers datasets torch torchvision torchaudio\n","!pip install pandas matplotlib tqdm scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0q_5jrUyQpXd"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Biblioth√®ques pour NLP moderne\n","from transformers import AutoTokenizer\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from tqdm.auto import tqdm\n","import math\n","import os\n","from typing import List\n","from timeit import default_timer as timer"]},{"cell_type":"markdown","metadata":{"id":"QbF8DpTAQpXe"},"source":["## Configuration et d√©tection mat√©riel\n","\n","**Concept important :** Il est crucial de d√©tecter si un GPU est disponible pour acc√©l√©rer l'entra√Ænement !"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTckZL6YQpXf"},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"üñ•Ô∏è  Device utilis√©: {DEVICE}\")\n","\n","# Configuration de la reproductibilit√©\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    # Configuration pour performances reproductibles sur GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"qkawsOUNQpXg"},"source":["## Hyperparam√®tres"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H_V_PqL4QpXh"},"outputs":[],"source":["EMB_SIZE = 256  # Dimension des embeddings (doit √™tre divisible par NHEAD)\n","NHEAD = 8       # Nombre de t√™tes d'attention (8 divise 256)\n","FFN_HID_DIM = 512  # Dimension de la couche feed-forward\n","\n","# Adaptation de la batch size selon les capacit√©s du device\n","BATCH_SIZE = 64 if DEVICE.type == 'cuda' else 32  # GPU peut traiter plus de donn√©es\n","\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","NUM_EPOCHS = 15  # Nombre d'√©poques d'entra√Ænement\n","LEARNING_RATE = 0.0001  # Taux d'apprentissage conservateur\n","\n","print(f\"üìä Batch size: {BATCH_SIZE}\")\n","print(f\"üß† Embedding size: {EMB_SIZE}\")\n","print(f\"üëÅÔ∏è  Attention heads: {NHEAD}\")\n","print(f\"üî• Epochs: {NUM_EPOCHS}\")"]},{"cell_type":"markdown","metadata":{"id":"BVwVfilOQpXi"},"source":["## Chargement des donn√©es\n","\n","**Dataset :** Nous utilisons un dataset de phrases parall√®les Anglais-Fran√ßais"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0-ItkaVQpXi"},"outputs":[],"source":["# Chargement du dataset (fourni)\n","try:\n","    import kagglehub\n","    path = kagglehub.dataset_download(\"devicharith/language-translation-englishfrench\")\n","    csv_path = os.path.join(path, 'eng_-french.csv')\n","except:\n","    # Alternative: mettre votre chemin local\n","    csv_path = 'eng_-french.csv'\n","\n","# CORRECTION: Chargement avec pandas et s√©lection des colonnes n√©cessaires\n","df = pd.read_csv(\n","    csv_path,\n","    usecols=['English words/sentences', 'French words/sentences']\n",")\n","df = df.dropna()  # Supprimer les valeurs manquantes\n","\n","print(f\"üìä Dataset size: {len(df)} exemples\")\n","# Affichage des premi√®res lignes pour v√©rification\n","print(df.head())"]},{"cell_type":"markdown","metadata":{"id":"BMatM4_eQpXj"},"source":["## Tokenisation moderne\n","\n","**Concept :** Les tokenizers pr√©-entra√Æn√©s sont plus efficaces que construire le sien from scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9wKCigxQpXj"},"outputs":[],"source":["# CORRECTION: Initialisation des tokenizers modernes\n","\n","from transformers import AutoTokenizer\n","\n","# Tokenizers pr√©-entra√Æn√©s optimis√©s\n","tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Anglais\n","tokenizer_fr = AutoTokenizer.from_pretrained(\"camembert-base\")     # Fran√ßais\n","\n","# Test des tokenizers\n","test_en = \"Hello I love machine learning\"\n","test_fr = \"Bonjour j'adore l'apprentissage automatique\"\n","\n","print(f\"üî§ Tokenization EN: {tokenizer_en.tokenize(test_en)}\")\n","print(f\"üî§ Tokenization FR: {tokenizer_fr.tokenize(test_fr)}\")\n","\n","print(\"‚úÖ Tokenizers initialis√©s !\")\n","\n","# Note: Ces tokenizers sont bien plus sophistiqu√©s que les tokenizers basiques\n","# Ils g√®rent les sous-mots (subwords) et ont √©t√© entra√Æn√©s sur de gros corpus"]},{"cell_type":"markdown","metadata":{"id":"l_ko9APLQpXj"},"source":["## Construction du vocabulaire"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rhfj-uVkQpXj"},"outputs":[],"source":["# Tokens sp√©ciaux (fourni)\n","PAD_IDX = 0  # Padding\n","BOS_IDX = 1  # Beginning of Sentence\n","EOS_IDX = 2  # End of Sentence\n","UNK_IDX = 3  # Unknown word\n","\n","def build_vocab(sentences, tokenizer, max_vocab=10000):\n","    \"\"\"Construction du vocabulaire √† partir des phrases\"\"\"\n","    word_count = {}\n","\n","    # CORRECTION: Parcours et comptage des mots\n","    for sentence in sentences:\n","        tokens = tokenizer.tokenize(str(sentence))\n","        for token in tokens:\n","            # Incr√©mentation du compteur pour chaque token\n","            word_count[token] = word_count.get(token, 0) + 1\n","\n","    # CORRECTION: Tri par fr√©quence d√©croissante\n","    sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Cr√©ation du vocabulaire avec tokens sp√©ciaux\n","    vocab = {\"<pad>\": PAD_IDX, \"<bos>\": BOS_IDX, \"<eos>\": EOS_IDX, \"<unk>\": UNK_IDX}\n","\n","    # CORRECTION: Ajout des mots les plus fr√©quents\n","    for word, _ in sorted_words[:max_vocab-4]:  # -4 pour les tokens sp√©ciaux\n","        vocab[word] = len(vocab)  # Assigne un index unique √† chaque mot\n","\n","    return vocab\n","\n","# Construction des vocabulaires\n","vocab_en = build_vocab(df['English words/sentences'], tokenizer_en)\n","vocab_fr = build_vocab(df['French words/sentences'], tokenizer_fr)\n","\n","print(f\"üìñ Vocabulaire EN: {len(vocab_en)} mots\")\n","print(f\"üìñ Vocabulaire FR: {len(vocab_fr)} mots\")"]},{"cell_type":"markdown","metadata":{"id":"8zMzN8fsQpXk"},"source":["## Dataset personnalis√©\n","\n","**Architecture :** PyTorch utilise des classes Dataset pour organiser les donn√©es"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1GbBVJoQpXk"},"outputs":[],"source":["def text_to_indices(text, tokenizer, vocab, max_len=50):\n","    \"\"\"Convertit un texte en indices avec BOS et EOS\"\"\"\n","    tokens = tokenizer.tokenize(str(text))[:max_len-2]  # Place pour BOS/EOS\n","\n","    # CORRECTION: Conversion des tokens en indices\n","    # Utilisation de vocab.get() pour g√©rer les mots inconnus\n","    indices = [BOS_IDX] + [vocab.get(token, UNK_IDX) for token in tokens] + [EOS_IDX]\n","\n","    return torch.tensor(indices, dtype=torch.long)\n","\n","class TranslationDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df.reset_index(drop=True)\n","\n","    def __len__(self):\n","        # CORRECTION: Retourner la taille du dataset\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        # CORRECTION: Extraction des textes anglais et fran√ßais\n","        en_text = self.df.iloc[idx]['English words/sentences']\n","        fr_text = self.df.iloc[idx]['French words/sentences']\n","\n","        # Conversion en indices\n","        en_indices = text_to_indices(en_text, tokenizer_en, vocab_en)\n","        fr_indices = text_to_indices(fr_text, tokenizer_fr, vocab_fr)\n","\n","        return en_indices, fr_indices\n","\n","print(\"‚úÖ Classes Dataset d√©finies !\")"]},{"cell_type":"markdown","metadata":{"id":"k1lSR7-tQpXk"},"source":["## DataLoaders\n","\n","**Concept :** Le padding permet de traiter des phrases de longueurs diff√©rentes dans le m√™me batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hu7xM6KVQpXk"},"outputs":[],"source":["def collate_batch(batch):\n","    \"\"\"Fonction pour cr√©er des batchs avec padding\"\"\"\n","    src_batch, tgt_batch = zip(*batch)\n","\n","    # CORRECTION: Utilisation de pad_sequence pour uniformiser les longueurs\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n","\n","    return src_batch, tgt_batch\n","\n","# CORRECTION: Division train/test avec scikit-learn\n","train_df, test_df = train_test_split(\n","    df,\n","    test_size=0.1,  # 10% pour le test\n","    random_state=SEED  # Reproductibilit√©\n",")\n","\n","# CORRECTION: Cr√©ation des datasets\n","train_dataset = TranslationDataset(train_df)\n","test_dataset = TranslationDataset(test_df)\n","\n","# CORRECTION: Cr√©ation des DataLoaders avec tous les param√®tres\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,  # M√©lange pour l'entra√Ænement\n","    collate_fn=collate_batch,\n","    num_workers=0\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,  # Pas de m√©lange pour l'√©valuation\n","    collate_fn=collate_batch,\n","    num_workers=0\n",")\n","\n","print(f\"üîÑ Training batches: {len(train_loader)}\")\n","print(f\"üîÑ Test batches: {len(test_loader)}\")"]},{"cell_type":"markdown","metadata":{"id":"dIfXO4BPQpXk"},"source":["## Architecture du mod√®le Transformer\n","\n","**Architecture :** Le Transformer utilise l'attention pour \"encoder\" les relations entre mots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fqm5FCZhQpXk"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    \"\"\"Encodage positionnel pour que le mod√®le comprenne l'ordre des mots\"\"\"\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # CORRECTION: Cr√©ation du tensor d'encodage positionnel\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","\n","        # Formule math√©matique pour l'encodage positionnel (from paper)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)  # Positions paires: sin\n","        pe[:, 1::2] = torch.cos(position * div_term)  # Positions impaires: cos\n","        pe = pe.unsqueeze(0)  # Ajout dimension batch\n","\n","        self.register_buffer('pe', pe)  # Buffer = tensor non-entra√Ænable\n","\n","    def forward(self, x):\n","        # CORRECTION: Addition de l'encodage positionnel aux embeddings\n","        x = x + self.pe[:, :x.size(1)]  # Prendre seulement la longueur n√©cessaire\n","        return self.dropout(x)\n","\n","print(\"‚úÖ PositionalEncoding d√©fini !\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWJF-48gQpXl"},"outputs":[],"source":["class TransformerTranslator(nn.Module):\n","    \"\"\"Mod√®le Transformer complet pour la traduction\"\"\"\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,\n","                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n","        super().__init__()\n","\n","        self.d_model = d_model\n","\n","        # CORRECTION: Couches d'embedding pour les vocabulaires source et target\n","        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n","        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n","\n","        # CORRECTION: Encodage positionnel\n","        self.pos_encoding = PositionalEncoding(d_model, dropout)\n","\n","        # CORRECTION: Architecture Transformer avec tous les param√®tres\n","        self.transformer = nn.Transformer(\n","            d_model=d_model,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout,\n","            batch_first=True  # Important: batch en premi√®re dimension\n","        )\n","\n","        # CORRECTION: Couche de projection finale vers le vocabulaire target\n","        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n","\n","        # Initialisation des poids (bonne pratique)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        \"\"\"Initialisation Xavier/Glorot pour meilleure convergence\n","        PyTorch utilise par d√©faut : Normal(0, 1) pour les embeddings\n","        PyTorch utilise : Uniform(-1/sqrt(in_features), 1/sqrt(in_features)) pour les Linear\n","        Les embeddings avec Normal(0,1) peuvent √™tre trop grands Uniform(-0.1, 0.1) plus stable pour l'attention\"\"\"\n","        initrange = 0.1\n","        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n","        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n","        self.output_projection.bias.data.zero_()\n","        self.output_projection.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n","                src_key_padding_mask=None, tgt_key_padding_mask=None):\n","\n","        # CORRECTION: Embeddings avec scaling (important pour la stabilit√©)\n","        src_emb = self.pos_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n","        tgt_emb = self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n","\n","        # CORRECTION: Passage dans le Transformer\n","        output = self.transformer(\n","            src_emb, tgt_emb,\n","            src_mask=src_mask,\n","            tgt_mask=tgt_mask,\n","            src_key_padding_mask=src_key_padding_mask,\n","            tgt_key_padding_mask=tgt_key_padding_mask\n","        )\n","\n","        # CORRECTION: Projection finale vers le vocabulaire\n","        return self.output_projection(output)\n","\n","print(\"‚úÖ Mod√®le Transformer d√©fini !\")"]},{"cell_type":"markdown","metadata":{"id":"8uJcXHk4QpXl"},"source":["## Cr√©ation des masques\n","\n","**Concept crucial :** Les masques emp√™chent le mod√®le de \"tricher\" en regardant les mots futurs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YkdHnLbeQpXl"},"outputs":[],"source":["def create_masks(src, tgt):\n","    \"\"\"Cr√©e les masques n√©cessaires pour l'attention\"\"\"\n","    src_seq_len = src.shape[1]\n","    tgt_seq_len = tgt.shape[1]\n","\n","    # CORRECTION: Masque causal triangulaire sup√©rieur\n","    # Emp√™che le d√©codeur de voir les mots futurs\n","    tgt_mask = torch.triu(\n","        torch.ones(tgt_seq_len, tgt_seq_len, device=src.device),\n","        diagonal=1  # Diagonale sup√©rieure = positions futures\n","    ).bool()  # Conversion en bool pour √©viter warning\n","\n","    # CORRECTION: Masques de padding pour ignorer les tokens PAD (type bool)\n","    src_padding_mask = (src == PAD_IDX)  # True o√π il y a du padding\n","    tgt_padding_mask = (tgt == PAD_IDX)  # True o√π il y a du padding\n","\n","    # Convertir tgt_mask en masque avec -inf pour compatibilit√©\n","    tgt_mask = tgt_mask.float().masked_fill(tgt_mask == 1, float('-inf')).masked_fill(tgt_mask == 0, 0.0)\n","\n","    return None, tgt_mask, src_padding_mask, tgt_padding_mask\n","\n","print(\"‚úÖ Fonction de masques d√©finie !\")"]},{"cell_type":"markdown","metadata":{"id":"36r21f1WQpXl"},"source":["## Initialisation du mod√®le"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ruba8qCkQpXl"},"outputs":[],"source":["# CORRECTION: Initialisation compl√®te du mod√®le\n","model = TransformerTranslator(\n","    src_vocab_size=len(vocab_en),\n","    tgt_vocab_size=len(vocab_fr),\n","    d_model=EMB_SIZE,\n","    nhead=NHEAD,\n","    num_encoder_layers=NUM_ENCODER_LAYERS,\n","    num_decoder_layers=NUM_DECODER_LAYERS,\n","    dim_feedforward=FFN_HID_DIM\n",").to(DEVICE)  # IMPORTANT: D√©placer sur le bon device!\n","\n","# CORRECTION: Comptage pr√©cis des param√®tres\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"üßÆ Param√®tres totaux: {total_params:,}\")\n","print(f\"üßÆ Param√®tres entra√Ænables: {trainable_params:,}\")\n","\n","# Estimation m√©moire approximative\n","memory_mb = (total_params * 4) / (1024**2)  # 4 bytes par param√®tre float32\n","print(f\"üíæ M√©moire approximative: {memory_mb:.1f} MB\")\n","\n","# Breakdown d√©taill√© des param√®tres\n","print(f\"\\nüìä D√©tail des composants:\")\n","print(f\"  üìñ Embeddings EN: {len(vocab_en) * EMB_SIZE:,} params\")\n","print(f\"  üìñ Embeddings FR: {len(vocab_fr) * EMB_SIZE:,} params\")\n","print(f\"  üîÑ Transformer: {total_params - len(vocab_en)*EMB_SIZE - len(vocab_fr)*EMB_SIZE - len(vocab_fr)*EMB_SIZE:,} params\")\n","print(f\"  üì§ Output projection: {EMB_SIZE * len(vocab_fr):,} params\")"]},{"cell_type":"markdown","metadata":{"id":"KD4anM_rQpXm"},"source":["## Configuration de l'entra√Ænement\n","\n","**Optimisation :** Le gradient clipping √©vite les explosions de gradient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3VFljoGfQpXm"},"outputs":[],"source":["# CORRECTION: Configuration compl√®te de l'entra√Ænement\n","\n","# Fonction de co√ªt qui ignore les tokens de padding\n","criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","# Optimiseur AdamW (version am√©lior√©e d'Adam)\n","optimizer = torch.optim.AdamW(\n","    model.parameters(),\n","    lr=LEARNING_RATE,\n","    betas=(0.9, 0.98),  # Coefficients recommand√©s pour Transformers\n","    eps=1e-9,\n","    weight_decay=0.01\n",")\n","\n","# Scheduler pour r√©duire le learning rate progressivement\n","scheduler = torch.optim.lr_scheduler.StepLR(\n","    optimizer,\n","    step_size=5,  # R√©duire tous les 5 √©poques\n","    gamma=0.7     # Facteur de r√©duction\n",")"]},{"cell_type":"markdown","metadata":{"id":"FgpCytHkQpXm"},"source":["## Fonctions d'entra√Ænement"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S37bIJEkQpXm"},"outputs":[],"source":["def train_epoch(model, train_loader, criterion, optimizer, device):\n","    \"\"\"Entra√Æne le mod√®le pour une √©poque\"\"\"\n","    # CORRECTION: Mode entra√Ænement (active dropout, batch norm)\n","    model.train()\n","\n","    total_loss = 0\n","    num_batches = len(train_loader)\n","\n","    for src, tgt in tqdm(train_loader, desc=\"Training\"):\n","        # CORRECTION: D√©placement des donn√©es sur le device\n","        src, tgt = src.to(device), tgt.to(device)\n","\n","        # CORRECTION: Pr√©paration teacher forcing\n","        # Input = toute la s√©quence sauf le dernier token\n","        # Output = toute la s√©quence sauf le premier token (BOS)\n","        tgt_input = tgt[:, :-1]  # [BOS, w1, w2, w3]\n","        tgt_output = tgt[:, 1:]  # [w1, w2, w3, EOS]\n","\n","        # CORRECTION: Cr√©ation des masques\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input)\n","\n","        # CORRECTION: Forward pass\n","        logits = model(\n","            src, tgt_input,\n","            src_mask, tgt_mask,\n","            src_padding_mask, tgt_padding_mask\n","        )\n","\n","        # CORRECTION: Calcul de la loss avec reshape appropri√©\n","        loss = criterion(\n","            logits.reshape(-1, logits.shape[-1]),  # (batch*seq, vocab)\n","            tgt_output.reshape(-1)                 # (batch*seq,)\n","        )\n","\n","        # CORRECTION: Backward pass avec gradient clipping\n","        optimizer.zero_grad()  # Reset gradients\n","        loss.backward()        # Calcul gradients\n","        # Gradient clipping pour √©viter l'explosion\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()       # Mise √† jour param√®tres\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / num_batches\n","\n","print(\"‚úÖ Fonction train_epoch d√©finie !\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQ1GW8I5QpXm"},"outputs":[],"source":["def evaluate(model, test_loader, criterion, device):\n","    \"\"\"√âvalue le mod√®le sur les donn√©es de test\"\"\"\n","    # CORRECTION: Mode √©valuation (d√©sactive dropout, batch norm)\n","    model.eval()\n","\n","    total_loss = 0\n","    num_batches = len(test_loader)\n","\n","    # CORRECTION: Pas de calcul de gradients pour √©conomiser m√©moire\n","    with torch.no_grad():\n","        for src, tgt in tqdm(test_loader, desc=\"Evaluating\"):\n","            # M√™me logique que l'entra√Ænement mais sans backward pass\n","            src, tgt = src.to(device), tgt.to(device)\n","\n","            tgt_input = tgt[:, :-1]\n","            tgt_output = tgt[:, 1:]\n","\n","            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input)\n","\n","            logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask)\n","            loss = criterion(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))\n","\n","            total_loss += loss.item()\n","\n","    return total_loss / num_batches\n","\n","print(\"‚úÖ Fonction evaluate d√©finie !\")"]},{"cell_type":"markdown","metadata":{"id":"oYTIHZqBQpXm"},"source":["## Boucle d'entra√Ænement principale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECyIYSHjQpXm"},"outputs":[],"source":["# CORRECTION: Boucle d'entra√Ænement compl√®te avec monitoring\n","\n","# Listes pour stocker l'historique\n","train_losses = []\n","val_losses = []\n","best_val_loss = float('inf')\n","\n","print(\"üöÄ D√©but de l'entra√Ænement !\")\n","print(\"=\" * 60)\n","\n","# CORRECTION: Boucle principale d'entra√Ænement\n","for epoch in range(NUM_EPOCHS):\n","    start_time = timer()\n","\n","    # CORRECTION: Phase d'entra√Ænement\n","    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n","\n","    # CORRECTION: Phase de validation\n","    val_loss = evaluate(model, test_loader, criterion, DEVICE)\n","\n","    # CORRECTION: Mise √† jour du learning rate\n","    scheduler.step()\n","\n","    end_time = timer()\n","\n","    # Sauvegarde de l'historique\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","\n","    # Affichage d√©taill√© des m√©triques\n","    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}:\")\n","    print(f\"  üìä Train Loss: {train_loss:.4f}\")\n","    print(f\"  üìä Val Loss: {val_loss:.4f}\")\n","    print(f\"  ‚è±Ô∏è  Time: {end_time - start_time:.2f}s\")\n","    print(f\"  üéöÔ∏è  LR: {scheduler.get_last_lr()[0]:.6f}\")\n","\n","    # D√©tection d'am√©lioration\n","    improvement = \"\"\n","    if len(val_losses) > 1:\n","        diff = val_losses[-2] - val_losses[-1]\n","        if diff > 0:\n","            improvement = f\" (‚Üì -{diff:.4f})\"\n","        else:\n","            improvement = f\" (‚Üë +{abs(diff):.4f})\"\n","\n","    print(f\"  üìà Progression: {improvement}\")\n","\n","    # CORRECTION: Sauvegarde du meilleur mod√®le\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'val_loss': val_loss,\n","            'train_loss': train_loss,\n","            'vocab_en': vocab_en,\n","            'vocab_fr': vocab_fr,\n","            'hyperparams': {\n","                'EMB_SIZE': EMB_SIZE,\n","                'NHEAD': NHEAD,\n","                'NUM_ENCODER_LAYERS': NUM_ENCODER_LAYERS,\n","                'NUM_DECODER_LAYERS': NUM_DECODER_LAYERS,\n","                'FFN_HID_DIM': FFN_HID_DIM\n","            }\n","        }, 'best_model_students_correction.pt')\n","        print(f\"  üéØ Nouveau meilleur mod√®le sauvegard√©! (val_loss: {val_loss:.4f})\")\n","\n","    # Early stopping simple\n","    if len(val_losses) > 3:\n","        if all(val_losses[i] <= val_losses[i+1] for i in range(-4, -1)):\n","            print(f\"  ‚ö†Ô∏è  Attention: val_loss stagne depuis 3 √©poques\")\n","\n","    print(\"-\" * 50)\n","\n","print(\"\\nüéâ Entra√Ænement termin√© !\")\n","print(f\"üèÜ Meilleure validation loss: {best_val_loss:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"wGG4u57SQpXn"},"source":["## Visualisation des r√©sultats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VTnxePS6QpXn"},"outputs":[],"source":["\n","plt.figure(figsize=(12, 8))\n","\n","# Subplot 1: Courbes de loss\n","plt.subplot(2, 2, 1)\n","plt.plot(range(1, len(train_losses) + 1), train_losses,\n","         label='Train Loss', color='blue', linewidth=2)\n","plt.plot(range(1, len(val_losses) + 1), val_losses,\n","         label='Validation Loss', color='red', linewidth=2)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('√âvolution des losses pendant l\\'entra√Ænement')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# Subplot 2: Diff√©rence train/val (d√©tection overfitting)\n","plt.subplot(2, 2, 2)\n","diff_losses = [t - v for t, v in zip(train_losses, val_losses)]\n","plt.plot(range(1, len(diff_losses) + 1), diff_losses,\n","         color='orange', linewidth=2)\n","plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n","plt.xlabel('Epochs')\n","plt.ylabel('Train Loss - Val Loss')\n","plt.title('√âcart Train/Validation (Overfitting)')\n","plt.grid(True, alpha=0.3)\n","\n","# Subplot 3: Learning rate √©volution\n","plt.subplot(2, 2, 3)\n","lrs = [LEARNING_RATE * (scheduler.gamma ** (i // scheduler.step_size))\n","       for i in range(len(train_losses))]\n","plt.plot(range(1, len(lrs) + 1), lrs, color='green', linewidth=2)\n","plt.xlabel('Epochs')\n","plt.ylabel('Learning Rate')\n","plt.title('√âvolution du Learning Rate')\n","plt.yscale('log')  # √âchelle logarithmique pour mieux voir\n","plt.grid(True, alpha=0.3)\n","\n","# Subplot 4: Am√©lioration par √©poque\n","plt.subplot(2, 2, 4)\n","val_improvements = [0] + [val_losses[i-1] - val_losses[i]\n","                          for i in range(1, len(val_losses))]\n","colors = ['green' if x > 0 else 'red' for x in val_improvements]\n","plt.bar(range(1, len(val_improvements) + 1), val_improvements, color=colors, alpha=0.7)\n","plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n","plt.xlabel('Epochs')\n","plt.ylabel('Am√©lioration Val Loss')\n","plt.title('Am√©lioration par √âpoque')\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Sauvegarde du graphique\n","plt.savefig('training_analysis_correction.png', dpi=300, bbox_inches='tight')\n","\n","# Analyse automatique des r√©sultats\n","print(\"üìà Analyse automatique des r√©sultats:\")\n","print(\"=\" * 50)\n","print(f\"  üèÜ Meilleure val loss: {best_val_loss:.4f}\")\n","print(f\"  üìä Loss finale train: {train_losses[-1]:.4f}\")\n","print(f\"  üìä Loss finale val: {val_losses[-1]:.4f}\")\n","print(f\"  üìâ R√©duction train loss: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")\n","print(f\"  üìâ R√©duction val loss: {((val_losses[0] - val_losses[-1]) / val_losses[0] * 100):.1f}%\")\n","\n","# D√©tection automatique des probl√®mes\n","final_gap = train_losses[-1] - val_losses[-1]\n","if final_gap > 0.5:\n","    print(f\"  ‚ö†Ô∏è  OVERFITTING d√©tect√© (√©cart: {final_gap:.3f})\")\n","elif final_gap < -0.2:\n","    print(f\"  ‚ö†Ô∏è  UNDERFITTING possible (√©cart: {final_gap:.3f})\")\n","else:\n","    print(f\"  ‚úÖ √âquilibre train/val correct (√©cart: {final_gap:.3f})\")\n","\n","# Convergence\n","if len(val_losses) > 5:\n","    recent_std = np.std(val_losses[-5:])\n","    if recent_std < 0.01:\n","        print(f\"  ‚úÖ Convergence stable (std r√©cente: {recent_std:.4f})\")\n","    else:\n","        print(f\"  üìä Convergence en cours (std r√©cente: {recent_std:.4f})\")"]},{"cell_type":"markdown","metadata":{"id":"SrW3q4alQpXo"},"source":["## Test du mod√®le\n","\n","**Le moment de v√©rit√© :** Votre mod√®le sait-il traduire ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZ67ZDigQpXo"},"outputs":[],"source":["# CORRECTION: Chargement du meilleur mod√®le\n","checkpoint = torch.load('best_model_students_correction.pt', map_location=DEVICE)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(f\"‚úÖ Meilleur mod√®le charg√© (√©poque {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n","\n","# Vocabulaire invers√© pour la conversion\n","idx_to_word_fr = {idx: word for word, idx in vocab_fr.items()}\n","\n","def translate_sentence(model, sentence, max_length=50, verbose=False):\n","    \"\"\"Traduit une phrase anglaise en fran√ßais avec d√©codage greedy\"\"\"\n","    model.eval()\n","\n","    with torch.no_grad():\n","        # CORRECTION: Pr√©processing de la phrase source\n","        src_indices = text_to_indices(sentence, tokenizer_en, vocab_en)\n","        src = src_indices.unsqueeze(0).to(DEVICE)\n","\n","        if verbose:\n","            print(f\"Source tokens: {[vocab_en_inv.get(idx.item(), '<unk>') for idx in src_indices]}\")\n","\n","        # CORRECTION: Initialisation avec BOS token\n","        tgt = torch.tensor([[BOS_IDX]], device=DEVICE)\n","\n","        # G√©n√©ration auto-r√©gressive mot par mot\n","        for step in range(max_length):\n","            # CORRECTION: Cr√©ation des masques pour l'√©tat actuel\n","            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt)\n","\n","            # CORRECTION: Pr√©diction du prochain token\n","            logits = model(\n","                src, tgt,\n","                src_mask, tgt_mask,\n","                src_padding_mask, tgt_padding_mask\n","            )\n","\n","            # CORRECTION: S√©lection du token le plus probable (greedy)\n","            next_token = logits[0, -1].argmax().unsqueeze(0).unsqueeze(0)\n","\n","            if verbose:\n","                probs = F.softmax(logits[0, -1], dim=0)\n","                top_tokens = probs.topk(3)\n","                print(f\"Step {step}: Top tokens: {[(idx_to_word_fr.get(idx.item(), '<unk>'), prob.item()) for idx, prob in zip(top_tokens.indices, top_tokens.values)]}\")\n","\n","            # Arr√™t si EOS g√©n√©r√©\n","            if next_token.item() == EOS_IDX:\n","                break\n","\n","            # CORRECTION: Ajout du nouveau token √† la s√©quence\n","            tgt = torch.cat([tgt, next_token], dim=1)\n","\n","        # CORRECTION: Conversion des indices vers les mots\n","        translation = []\n","        for idx in tgt[0][1:]:  # Ignorer BOS au d√©but\n","            word = idx_to_word_fr.get(idx.item(), '<unk>')\n","            if word in ['<eos>', '<pad>']:\n","                break\n","            translation.append(word)\n","\n","        # Post-processing basique pour am√©liorer la lisibilit√©\n","        result = ' '.join(translation)\n","        # Corrections de ponctuation basiques\n","        result = result.replace(' ,', ',').replace(' .', '.').replace(' !', '!').replace(' ?', '?').replace('_', '')\n","\n","        return result\n","\n","print(\"‚úÖ Fonction de traduction d√©finie !\")\n","\n","# Vocabulaire invers√© pour debug\n","vocab_en_inv = {idx: word for word, idx in vocab_en.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZ15Xeb7QpXo"},"outputs":[],"source":["# CORRECTION: Tests complets avec analyse de qualit√©\n","\n","test_sentences = [\n","    \"Hello, how are you?\",\n","    \"I love machine learning.\",\n","    \"The weather is beautiful today.\",\n","    \"Thank you very much.\",\n","    \"Good morning!\",\n","    \"What is your name?\",\n","    \"I am learning French.\",\n","    \"The book is on the table.\",\n","    \"Where are you going?\",\n","    \"This is a difficult problem.\"\n","]\n","\n","# Traductions de r√©f√©rence pour comparaison\n","references = [\n","    \"Bonjour, comment allez-vous ?\",\n","    \"J'adore l'apprentissage automatique.\",\n","    \"Le temps est beau aujourd'hui.\",\n","    \"Merci beaucoup.\",\n","    \"Bonjour !\",\n","    \"Comment vous appelez-vous ?\",\n","    \"J'apprends le fran√ßais.\",\n","    \"Le livre est sur la table.\",\n","    \"O√π allez-vous ?\",\n","    \"C'est un probl√®me difficile.\"\n","]\n","\n","print(\"üéØ Test de traduction avec √©valuation:\")\n","print(\"=\" * 70)\n","\n","total_score = 0\n","for i, sentence in enumerate(test_sentences):\n","    translation = translate_sentence(model, sentence)\n","\n","    print(f\"\\nüìù Test {i+1}:\")\n","    print(f\"üá¨üáß EN: {sentence}\")\n","    print(f\"ü§ñ AI: {translation}\")\n","    print(f\"üìö REF: {references[i]}\")\n","\n","    # √âvaluation qualitative simple\n","    prediction_words = set(translation.lower().split())\n","    reference_words = set(references[i].lower().split())\n","\n","    if prediction_words and reference_words:\n","        overlap = len(prediction_words & reference_words)\n","        union = len(prediction_words | reference_words)\n","        jaccard = overlap / union if union > 0 else 0\n","        total_score += jaccard\n","\n","        if jaccard > 0.6:\n","            quality = \"üü¢ Excellent\"\n","        elif jaccard > 0.4:\n","            quality = \"üü° Correct\"\n","        elif jaccard > 0.2:\n","            quality = \"üü† Partiel\"\n","        else:\n","            quality = \"üî¥ Faible\"\n","\n","        print(f\"üìä Similarit√©: {jaccard:.2f} {quality}\")\n","\n","    print(\"-\" * 40)\n","\n","average_score = total_score / len(test_sentences)\n","print(f\"\\nüèÜ Score moyen: {average_score:.3f}\")\n","\n","if average_score > 0.5:\n","    print(\"üéâ Performance globale: BONNE !\")\n","elif average_score > 0.3:\n","    print(\"üëç Performance globale: Correcte\")\n","else:\n","    print(\"üìö Performance globale: √Ä am√©liorer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubR_3zomQpXo"},"outputs":[],"source":["# Test interactif\n","\n","print(\"‚ú® Test interactif - Ajoutez vos propres phrases:\")\n","print(\"=\" * 50)\n","\n","# Vos phrases personnalis√©es que vous pouvez modifier\n","custom_sentences = [\n","    \"The cat is sleeping on the sofa.\",\n","    \"I want to travel to Paris.\",\n","    \"Can you help me please?\",\n","    \"The students are working hard.\",\n","    \"Technology is changing the world.\"\n","]\n","\n","for i, sentence in enumerate(custom_sentences):\n","    print(f\"\\nüß™ Test personnalis√© {i+1}:\")\n","    translation = translate_sentence(model, sentence)\n","    print(f\"üá¨üáß EN: {sentence}\")\n","    print(f\"üá´üá∑ FR: {translation}\")\n","\n","    # Analyse d√©taill√©e pour 1 exemple\n","    if i == 0:\n","        print(\"\\nüîç Analyse d√©taill√©e:\")\n","        detailed_translation = translate_sentence(model, sentence, verbose=True)"]},{"cell_type":"code","source":["\n"],"metadata":{"id":"BVoQ4qlfSLHt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LPaz2472V_Jz"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}